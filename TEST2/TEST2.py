import tkinter as tk
from tkinter import filedialog, messagebox, ttk
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import csv
import os

# --- è¨­å®šã¨ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯ ---

def get_session(url):
    """
    requests.Sessionã‚’ä½œæˆã—ã€ãƒ™ãƒ¼ã‚¹URLã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’è©¦ã¿ã‚‹ (èªè¨¼ã¯çœç•¥/å¿…è¦ã«å¿œã˜ã¦æ‹¡å¼µ)
    """
    session = requests.Session()
    try:
        # èªè¨¼ãŒå¿…è¦ãªå ´åˆã¯ã€ã“ã“ã§ãƒ­ã‚°ã‚¤ãƒ³POSTãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å®Ÿè£…ã™ã‚‹
        session.get(url, timeout=5)
        return session
    except requests.RequestException:
        return None

def analyze_structure(base_url):
    """
    ãƒ™ãƒ¼ã‚¹URLã‹ã‚‰ä¸»è¦ãªãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ï¼ˆåˆ†é¡ï¼‰ã‚’æŠ½å‡ºã—ã€ãƒªã‚¹ãƒˆã‚’è¿”ã™
    """
    try:
        session = get_session(base_url)
        if not session:
            raise Exception("ãƒ™ãƒ¼ã‚¹URLã¸ã®æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

        response = session.get(base_url, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        # ã‚µã‚¤ãƒˆã®ä¸»è¦ãªãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³é ˜åŸŸã‚’æŒ‡å®š (å¯¾è±¡ã‚µã‚¤ãƒˆã«åˆã‚ã›ã¦èª¿æ•´ãŒå¿…è¦)
        # ä¾‹: <nav>ã‚¿ã‚°å†…ã®ãƒªãƒ³ã‚¯ã€id="sidebar"å†…ã®ãƒªãƒ³ã‚¯ã€class="main-menu"å†…ã®ãƒªãƒ³ã‚¯
        nav_elements = soup.select('nav a[href], #sidebar a[href], .main-menu a[href], .navbar a[href]')

        major_paths = set()
        base_netloc = urlparse(base_url).netloc

        for link in nav_elements:
            href = link.get('href')
            full_url = urljoin(base_url, href)
            parsed_url = urlparse(full_url)

            # 1. åŒã˜ãƒ›ã‚¹ãƒˆã§ã‚ã‚‹ã“ã¨
            # 2. ãƒ­ã‚°ã‚¤ãƒ³/ãƒ­ã‚°ã‚¢ã‚¦ãƒˆãƒšãƒ¼ã‚¸ã§ã¯ãªã„ã“ã¨
            # 3. æ‹¡å¼µå­ã‚’æŒã¤ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆç”»åƒã€PDFãªã©ï¼‰ã§ã¯ãªã„ã“ã¨
            if (parsed_url.netloc == base_netloc and
                not parsed_url.path.lower().startswith(('/login', '/logout', '/signout')) and
                not parsed_url.path.split('/')[-1].count('.')):

                path = parsed_url.path.rstrip('/')

                if path and path != urlparse(base_url).path.rstrip('/'):
                    # ãƒ‘ã‚¹ã®ãƒ«ãƒ¼ãƒˆã«ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’è¿½åŠ ã—ã¦ã€ä¸€è²«æ€§ã‚’æŒãŸã›ã‚‹
                    major_paths.add(path if path.startswith('/') else '/' + path)

        return sorted(list(major_paths))

    except Exception as e:
        raise Exception(f"æ§‹é€ åˆ†æã‚¨ãƒ©ãƒ¼: {e}")

def analyze_and_crawl(session, base_url, target_classification_path):
    """
    æŒ‡å®šã•ã‚ŒãŸåˆ†é¡ãƒ‘ã‚¹ä»¥ä¸‹ã®ãƒšãƒ¼ã‚¸ã‚’å·¡å›ã—ã€ãƒ‡ãƒ¼ã‚¿ã¨æ¬¡ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã™ã‚‹
    """
    start_url = urljoin(base_url, target_classification_path)
    crawl_queue = [start_url]
    visited_urls = {start_url}
    pages_data = []

    while crawl_queue and len(pages_data) < 100: # å¿µã®ãŸã‚æœ€å¤§100ãƒšãƒ¼ã‚¸ã«åˆ¶é™
        current_url = crawl_queue.pop(0)

        try:
            response = session.get(current_url, timeout=10)
            if response.status_code != 200:
                continue

            soup = BeautifulSoup(response.text, 'html.parser')
            page_title = soup.title.string.strip() if soup.title else 'No Title'

            # --- ğŸ’¡ ãƒ‡ãƒ¼ã‚¿å–å¾—ã®ã‚³ã‚¢ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆãƒ•ã‚©ãƒ¼ãƒ ã‚„è¨­å®šå€¤ãªã©ï¼‰ ---
            # ç°¡æ½”ã«ã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ã‚¿ã‚¤ãƒˆãƒ«ã¨ä¸»è¦ãªè¦‹å‡ºã—ã‚’æŠ½å‡º

            # ä¸»è¦è¦‹å‡ºã— (H1) ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            h1 = soup.find('h1')
            main_heading = h1.get_text(strip=True)[:50] if h1 else 'N/A'

            # å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
            pages_data.append({
                'URL': current_url,
                'Title': page_title,
                'Main_Heading': main_heading,
                'Status': response.status_code,
            })
            # --------------------------------------------------------

            # ãƒªãƒ³ã‚¯ã®æŠ½å‡ºã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (ãƒ„ãƒªãƒ¼æ§‹é€ ã®åˆ†æ)
            base_netloc = urlparse(base_url).netloc
            for link in soup.find_all('a', href=True):
                href = link['href']
                full_url = urljoin(current_url, href)
                parsed_url = urlparse(full_url)

                # 1. å·¡å›æ¸ˆã¿ã§ãªã„ã“ã¨
                # 2. ãƒ™ãƒ¼ã‚¹URLã¨åŒã˜ãƒ›ã‚¹ãƒˆã§ã‚ã‚‹ã“ã¨
                # 3. æŒ‡å®šã•ã‚ŒãŸã€Œåˆ†é¡ã€ãƒ‘ã‚¹ã§å§‹ã¾ã‚‹ã“ã¨
                if (full_url not in visited_urls and
                    parsed_url.netloc == base_netloc and
                    parsed_url.path.startswith(target_classification_path)):

                    visited_urls.add(full_url)
                    crawl_queue.append(full_url)

        except requests.RequestException:
            continue

    return pages_data

# --- GUIã‚¯ãƒ©ã‚¹ ---

class WebCrawlerApp:
    def __init__(self, master):
        self.master = master
        master.title("ã‚¦ã‚§ãƒ–ç®¡ç†ç”»é¢ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ„ãƒ¼ãƒ«")

        # UIè¦ç´ ã®é…ç½®

        # 1. URLå…¥åŠ›
        tk.Label(master, text="ãƒ™ãƒ¼ã‚¹URL:").grid(row=0, column=0, sticky='w', padx=5, pady=5)
        self.url_entry = tk.Entry(master, width=50)
        self.url_entry.grid(row=0, column=1, padx=5, pady=5)
        self.url_entry.insert(0, "https://example.com/admin")

        # 2. æ§‹é€ åˆ†æãƒœã‚¿ãƒ³
        self.analyze_button = tk.Button(master, text="ã‚µã‚¤ãƒˆæ§‹é€ ã‚’åˆ†æ (åˆ†é¡ãƒªã‚¹ãƒˆå–å¾—)", command=self.start_analysis, bg='#ADD8E6')
        self.analyze_button.grid(row=1, column=0, columnspan=2, pady=10)

        # 3. åˆ†é¡é¸æŠ (Combobox)
        tk.Label(master, text="åˆ†é¡ã‚’é¸æŠ:").grid(row=2, column=0, sticky='w', padx=5, pady=5)
        self.path_combobox = ttk.Combobox(master, state="readonly", width=48)
        self.path_combobox.grid(row=2, column=1, padx=5, pady=5)

        # 4. å®Ÿè¡Œãƒœã‚¿ãƒ³
        self.crawl_button = tk.Button(master, text="ãƒ‡ãƒ¼ã‚¿å–å¾—ã¨CSVå‡ºåŠ›", command=self.start_crawl, bg='#90EE90', state=tk.DISABLED)
        self.crawl_button.grid(row=3, column=0, columnspan=2, pady=10)

        # 5. ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º
        self.status_label = tk.Label(master, text="å¾…æ©Ÿä¸­...")
        self.status_label.grid(row=4, column=0, columnspan=2, pady=5)

        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ä¸­å¤®ã«é…ç½®
        self.center_window()

    def center_window(self):
        self.master.update_idletasks()
        width = self.master.winfo_width()
        height = self.master.winfo_height()
        x = (self.master.winfo_screenwidth() // 2) - (width // 2)
        y = (self.master.winfo_screenheight() // 2) - (height // 2)
        self.master.geometry(f'{width}x{height}+{x}+{y}')

    def start_analysis(self):
        """æ§‹é€ åˆ†æãƒ•ã‚§ãƒ¼ã‚ºã‚’å®Ÿè¡Œã—ã€åˆ†é¡ãƒªã‚¹ãƒˆã‚’Comboboxã«è¨­å®šã™ã‚‹"""
        base_url = self.url_entry.get().strip().rstrip('/')
        if not base_url:
            messagebox.showwarning("å…¥åŠ›ã‚¨ãƒ©ãƒ¼", "ãƒ™ãƒ¼ã‚¹URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            return

        self.status_label.config(text="å‡¦ç†ä¸­... ã‚µã‚¤ãƒˆæ§‹é€ ã‚’åˆ†æã—ã¦ã„ã¾ã™ã€‚")
        self.master.update()

        try:
            # æ§‹é€ åˆ†æé–¢æ•°ã‚’å‘¼ã³å‡ºã—
            major_paths = analyze_structure(base_url)

            if major_paths:
                self.path_combobox['values'] = major_paths
                self.path_combobox.set(major_paths[0])
                self.crawl_button.config(state=tk.NORMAL)
                self.status_label.config(text=f"åˆ†æå®Œäº†ã€‚{len(major_paths)}å€‹ã®åˆ†é¡ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚é¸æŠã—ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            else:
                self.path_combobox['values'] = []
                self.path_combobox.set("")
                self.crawl_button.config(state=tk.DISABLED)
                self.status_label.config(text="åˆ†æå®Œäº† (åˆ†é¡ãƒ‘ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚URLãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„)")

        except Exception as e:
            messagebox.showerror("åˆ†æã‚¨ãƒ©ãƒ¼", str(e))
            self.status_label.config(text="ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")

    def start_crawl(self):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé¸æŠã—ãŸåˆ†é¡ãƒ‘ã‚¹ã§å·¡å›ã‚’é–‹å§‹ã—ã€CSVã«å‡ºåŠ›ã™ã‚‹"""
        base_url = self.url_entry.get().strip().rstrip('/')
        selected_path = self.path_combobox.get().strip()

        if not selected_path:
            messagebox.showwarning("é¸æŠã‚¨ãƒ©ãƒ¼", "åˆ†é¡ãƒ‘ã‚¹ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            return

        self.status_label.config(text=f"å‡¦ç†ä¸­... {selected_path}ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ã„ã¾ã™ã€‚")
        self.master.update()

        # ãƒ­ã‚°ã‚¤ãƒ³ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ç¢ºç«‹
        session = get_session(base_url)
        if not session:
            messagebox.showerror("æ¥ç¶šã‚¨ãƒ©ãƒ¼", "ã‚»ãƒƒã‚·ãƒ§ãƒ³ç¢ºç«‹/æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            self.status_label.config(text="ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
            return

        try:
            # ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°é–‹å§‹
            data_results = analyze_and_crawl(session, base_url, selected_path)

            if data_results:
                self.save_to_csv(data_results, selected_path)
            else:
                messagebox.showinfo("çµæœ", "æŒ‡å®šã•ã‚ŒãŸåˆ†é¡ãƒ‘ã‚¹ã§ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                self.status_label.config(text="å®Œäº† (ãƒ‡ãƒ¼ã‚¿ãªã—)")

        except Exception as e:
            messagebox.showerror("è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼", f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            self.status_label.config(text="ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")

    def save_to_csv(self, data, path_name):
        """å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã™ã‚‹"""
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’é–‹ã
        clean_path = path_name.strip('/').replace('/', '_')
        default_filename = f"crawl_data_{clean_path}.csv"

        filepath = filedialog.asksaveasfilename(
            defaultextension=".csv",
            initialfile=default_filename,
            filetypes=[("CSV files", "*.csv")]
        )

        if not filepath:
            self.status_label.config(text="CSVä¿å­˜ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸã€‚")
            return

        try:
            fieldnames = list(data[0].keys())
            with open(filepath, 'w', newline='', encoding='utf-8-sig') as csvfile: # utf-8-sigã§Excelã§ã®æ–‡å­—åŒ–ã‘é˜²æ­¢
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(data)

            messagebox.showinfo("å®Œäº†", f"ãƒ‡ãƒ¼ã‚¿ã‚’ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ:\n{filepath}")
            self.status_label.config(text=f"å®Œäº† ({len(data)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜)")

        except Exception as e:
            messagebox.showerror("CSVã‚¨ãƒ©ãƒ¼", f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã®æ›¸ãè¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚\nã‚¨ãƒ©ãƒ¼: {e}")
            self.status_label.config(text="ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
if __name__ == '__main__':
    root = tk.Tk()
    app = WebCrawlerApp(root)
    root.mainloop()